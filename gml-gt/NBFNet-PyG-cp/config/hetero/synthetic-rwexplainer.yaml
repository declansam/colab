output_dir: /storage/ryoji/Graph-Transformer/NBFNet-PyG/Hetero/RWExplainer/

dataset:
  class: synthetic
  root: /storage/ryoji/Graph-Transformer/NBFNet-PyG/datasets/hetero_dataset/

wandb:
  use: {{ use_wandb }}
  entity: "team_ryoji"
  project: SampleX-UserItemAttr
  name: RWExplainer

model:
  class: NBFNet
  input_dim: 32
  hidden_dims: [32, 32, 32]
  message_func: distmult
  aggregate_func: pna
  short_cut: yes
  layer_norm: yes
  dependent: yes
  randomized_edge_drop: 0.0
  use_pyg_propagation: yes

explainer:
  class: RelPGExplainer
  size_reg: 1
  ent_reg: 3
  temp_start: 4
  temp_end: 3
  sample_bias: 0.5
  rel_emb: yes
  num_mlp_layer: 4
  use_default_aggr: no
  topk_tails: 3
  random_walk_loss: yes
  adj_aggr: sum
  teleport_prob: 0.2
  rw_topk_node: 100
  reg_loss_inside: 1
  reg_loss_outside: 1

explainer_eval: # the evaluation of the explainers
  # eval_mask_type: hard_edge_mask_top_ratio
  eval_mask_type: hard_edge_mask_top_k
  hard_edge_mask_threshold: 0.5
  hard_edge_mask_top_k: [100, 300, 500]
  # hard_node_mask_top_k: [100]
  # hard_edge_mask_top_ratio: [0.001,0.01,0.1,0.3]

task:
  sample_neg_edges: yes
  metric: [auroc]

optimizer:
  class: Adam
  lr: 5.0e-3

eval:
  randomized_edge_drop: [0, 0.2, 0.4, 0.6, 0.8, 1]
  eval_on_edge_drop: yes
  remove_ground_truth: yes
  keep_ground_truth: yes

train:
  gpus: {{ gpus }}
  batch_size: 32
  num_epoch: 10
  log_interval: 100

checkpoint: /storage/ryoji/Graph-Transformer/NBFNet-PyG/checkpoint/synthetic-nbfnet.pth