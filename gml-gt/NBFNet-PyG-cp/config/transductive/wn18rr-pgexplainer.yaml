output_dir: /storage/ryoji/Graph-Transformer/NBFNet-PyG/explanation/
# output_dir: /scratch/rk3570/Graph-Transformer/NBFNet-PyG/experiments/

dataset:
  class: WN18RR
  root: /storage/ryoji/Graph-Transformer/NBFNet-PyG/datasets/knowledge_graphs/WN18RR/
  # root: /scratch/rk3570/Graph-Transformer/NBFNet-PyG/datasets/knowledge_graphs/WN18RR/

model:
  class: NBFNet
  input_dim: 32
  hidden_dims: [32, 32, 32, 32, 32, 32]
  message_func: distmult
  aggregate_func: pna
  short_cut: yes
  layer_norm: yes
  dependent: no
  num_beam: 5
  path_topk: 5
  # get_path: True

explainer:
  class: PGExplainer
  epochs: 100
  lr: 0.01
  reg_coefs: [0.03, 1]
  temp: [5.0, 1.0]
  sample_bias: 0.0
  hops: 6
  mode: transductive # inductive or transductive mode. 
  # if inductive, it will train an explainer on the train set and then use that explainer for val and test set.

task:
  num_negative: 32
  strict_negative: yes
  adversarial_temperature: 1
  metric: [mr, mrr, hits@1, hits@3, hits@10, hits@50, hits@100]

optimizer:
  class: Adam
  lr: 5.0e-3

train:
  gpus: {{ gpus }}
  batch_size: 16
  num_epoch: 0
  log_interval: 100

topk: 300
evaluate_on_train: True

checkpoint: /storage/ryoji/Graph-Transformer/NBFNet-PyG/checkpoint/wn18rr_transductive_best.pth
# checkpoint: /scratch/rk3570/Graph-Transformer/NBFNet-PyG/checkpoint/WN18RR_transductive_best.pth