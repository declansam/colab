# output_dir: /storage/ryoji/Graph-Transformer/NBFNet-PyG/PaGELink/
output_dir: /scratch/rk3570/Graph-Transformer/NBFNet-PyG/PaGELink/

dataset:
  class: FB15k-237
  # root: /storage/ryoji/Graph-Transformer/NBFNet-PyG/datasets/knowledge_graphs/WN18RR/
  root: /scratch/rk3570/Graph-Transformer/NBFNet-PyG/datasets/knowledge_graphs/FB15k-237/
  # combined_graph: yes

wandb:
  use: yes
  entity: "team_ryoji"
  project: SampleX-FB15k-237
  name: PaGELink-Hypersearch-Jubail

model:
  class: NBFNet
  input_dim: 32
  hidden_dims: [32, 32, 32, 32, 32, 32]
  message_func: distmult
  aggregate_func: pna
  short_cut: yes
  layer_norm: yes
  dependent: yes
  remove_one_hop: yes
  use_pyg_propagation: yes

explainer:
  class: PaGELink
  prune_graph: yes
  with_path_loss: yes
  timeout_duration: 0.1
  topk_tails: 1
  prune_max_degree: 50
  k_core: 2
  num_paths: 20
  alpha: 1
  beta: 1


explainer_eval: # the evaluation of the explainers
  # eval_mask_type: hard_edge_mask_top_ratio
  eval_mask_type: hard_edge_mask_top_k
  hard_edge_mask_threshold: 0.5
  hard_edge_mask_top_k: [100, 300, 500]
  # hard_edge_mask_top_ratio: [0.001,0.01,0.1,0.3]

task:
  num_negative: 32
  strict_negative: yes
  adversarial_temperature: 1
  metric: [mr, mrr, hits@1, hits@3, hits@10, hits@50, hits@100, inclusion, num_edges, num_nodes]

optimizer:
  class: Adam
  lr: 1.0e-3

train:
  gpus: [0]
  batch_size: 4
  num_epoch: 20
  log_interval: 100
  num_workers: 4
  step: 1

# checkpoint: /storage/ryoji/Graph-Transformer/NBFNet-PyG/checkpoint/wn18rr_02b.pth
checkpoint: /scratch/rk3570/Graph-Transformer/NBFNet-PyG/checkpoint/fb15k237_02b.pth