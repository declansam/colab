output_dir: /scratch/oge208/Graph-Transformer/NBFNet-PyG/explanation/CF-GNNExplainer/
# output_dir: /scratch/oge208/Graph-Transformer/NBFNet-PyG/Hetero/

dataset:
  class: WN18RR
  root: /scratch/oge208/Graph-Transformer/NBFNet-PyG/datasets/knowledge_graphs/
  # root: /scratch/oge208/Graph-Transformer/NBFNet-PyG/datasets/hetero_dataset/

wandb:
  use: {{ use_wandb }}
  entity: "team_ryoji"
  project: SampleX-WN18RR
  name: CF-GNNExplainer

model:
  class: NBFNet
  input_dim: 32
  hidden_dims: [32, 32, 32, 32, 32, 32]
  message_func: distmult
  aggregate_func: pna
  short_cut: yes
  layer_norm: yes
  dependent: no
  use_pyg_propagation: yes

explainer:
  class: GNNExplainer
  size_reg: 3
  ent_reg: 4
  topk_tails: 3
  ego_network: yes
  use_default_aggr: no
  factual: no
  counter_factual: yes

explainer_eval: # the evaluation of the explainers
  eval_mask_type: hard_edge_mask_top_k
  hard_edge_mask_top_k: [100, 300, 500, 1000, 5000]
  eval_type: [factual_eval]

task:
  num_negative: 32
  strict_negative: yes
  adversarial_temperature: 1
  metric: [mr, mrr, hits@1, hits@3, hits@10, inclusion, num_edges, num_nodes]

optimizer:
  class: Adam
  lr: 0.01

train:
  gpus: {{ gpus }}
  batch_size: 16
  num_epoch: 40
  log_interval: 100
  num_workers: 4
  step: 1

checkpoint: /scratch/oge208/Graph-Transformer/NBFNet-PyG/checkpoint/explain/wn18rr_0b.pth
eval_checkpoint: /scratch/oge208/Graph-Transformer/NBFNet-PyG/checkpoint/explain/wn18rr_015b.pth

save_explanation: yes
evaluate_on_train: yes